I0426 01:07:09.113703  2397 caffe.cpp:113] Use GPU with device ID 0
I0426 01:07:23.947792  2397 caffe.cpp:121] Starting Optimization
I0426 01:07:23.979671  2397 solver.cpp:32] Initializing solver from parameters: 
test_iter: 64
test_interval: 1000
base_lr: 0.01
display: 500
max_iter: 50000
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
stepsize: 50000
solver_mode: GPU
net: "/home/nitini/eas_499_code/network_architectures/13_seaNet_train_test.prototxt"
I0426 01:07:23.979727  2397 solver.cpp:70] Creating training net from net file: /home/nitini/eas_499_code/network_architectures/13_seaNet_train_test.prototxt
I0426 01:07:24.138154  2397 net.cpp:257] The NetState phase (0) differed from the phase (1) specified by a rule in layer ndsb
I0426 01:07:24.138217  2397 net.cpp:257] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0426 01:07:24.138489  2397 net.cpp:42] Initializing net from parameters: 
name: "SeaNet"
state {
  phase: TRAIN
}
layer {
  name: "ndsb"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
    mean_file: "./train_all_48_mean.binaryproto"
  }
  data_param {
    source: "/home/nitini/data_files/48_cross_val_files/cv_training_lmdb"
    batch_size: 256
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 128
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "reLU1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 128
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "reLU2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "dropout1"
  type: "Dropout"
  bottom: "norm2"
  top: "norm2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 256
    kernel_size: 2
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "reLU3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "norm3"
  type: "LRN"
  bottom: "conv3"
  top: "norm3"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "norm3"
  top: "conv4"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 256
    kernel_size: 2
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv4"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "norm4"
  type: "LRN"
  bottom: "pool1"
  top: "norm4"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "dropout2"
  type: "Dropout"
  bottom: "norm4"
  top: "norm4"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "norm4"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "reLU4"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "dropout3"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 256
    weight_filler {
      type: "xavier"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "reLU5"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "dropout4"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 121
    weight_filler {
      type: "xavier"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0426 01:07:24.138660  2397 layer_factory.hpp:74] Creating layer ndsb
I0426 01:07:24.141043  2397 net.cpp:84] Creating Layer ndsb
I0426 01:07:24.141062  2397 net.cpp:338] ndsb -> data
I0426 01:07:24.141095  2397 net.cpp:338] ndsb -> label
I0426 01:07:24.141115  2397 net.cpp:113] Setting up ndsb
I0426 01:07:24.678274  2397 db.cpp:34] Opened lmdb /home/nitini/data_files/48_cross_val_files/cv_training_lmdb
I0426 01:07:28.021225  2397 data_layer.cpp:67] output data size: 256,3,70,70
I0426 01:07:28.021273  2397 data_transformer.cpp:22] Loading mean file from: ./train_all_48_mean.binaryproto
I0426 01:07:29.092888  2397 net.cpp:120] Top shape: 256 3 70 70 (3763200)
I0426 01:07:29.092927  2397 net.cpp:120] Top shape: 256 (256)
I0426 01:07:29.092938  2397 layer_factory.hpp:74] Creating layer conv1
I0426 01:07:29.092963  2397 net.cpp:84] Creating Layer conv1
I0426 01:07:29.092973  2397 net.cpp:380] conv1 <- data
I0426 01:07:29.092993  2397 net.cpp:338] conv1 -> conv1
I0426 01:07:29.093009  2397 net.cpp:113] Setting up conv1
F0426 01:07:29.394249  2417 data_transformer.cpp:59] Check failed: datum_height == data_mean_.height() (70 vs. 48) 
*** Check failure stack trace: ***
    @     0x2afbd92d1e6d  (unknown)
    @     0x2afbd92d3ced  (unknown)
    @     0x2afbd92d1a5c  (unknown)
    @     0x2afbd92d463e  (unknown)
    @     0x2afbd32c2ccd  caffe::DataTransformer<>::Transform()
    @     0x2afbd32c4065  caffe::DataTransformer<>::Transform()
    @     0x2afbd32ff568  caffe::DataLayer<>::InternalThreadEntry()
    @     0x2afbdb5d124a  (unknown)
    @     0x2afbde362df5  start_thread
    @     0x2afbde66d1ad  __clone
/var/sge/default/spool/aws-foster-02/job_scripts/61973: line 5:  2397 Aborted                 caffe train --solver=/home/nitini/eas_499_code/network_architectures/seaNet_solver_all.prototxt
