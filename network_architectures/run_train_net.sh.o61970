I0425 23:46:35.548625  2388 caffe.cpp:113] Use GPU with device ID 0
I0425 23:46:43.292107  2388 caffe.cpp:121] Starting Optimization
I0425 23:46:43.324177  2388 solver.cpp:32] Initializing solver from parameters: 
test_iter: 64
test_interval: 1000
base_lr: 0.01
display: 500
max_iter: 50000
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
stepsize: 50000
solver_mode: GPU
net: "/home/nitini/eas_499_code/network_architectures/11_seaNet_train_test.prototxt"
I0425 23:46:43.324218  2388 solver.cpp:70] Creating training net from net file: /home/nitini/eas_499_code/network_architectures/11_seaNet_train_test.prototxt
I0425 23:46:43.417879  2388 net.cpp:257] The NetState phase (0) differed from the phase (1) specified by a rule in layer ndsb
I0425 23:46:43.417917  2388 net.cpp:257] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0425 23:46:43.418118  2388 net.cpp:42] Initializing net from parameters: 
name: "SeaNet"
state {
  phase: TRAIN
}
layer {
  name: "ndsb"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
    mean_file: "./train_all_48_mean.binaryproto"
  }
  data_param {
    source: "/home/nitini/data_files/48_cross_val_files/cv_training_lmdb"
    batch_size: 256
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 128
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "reLU1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 128
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "reLU2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "dropout1"
  type: "Dropout"
  bottom: "norm2"
  top: "norm2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 256
    kernel_size: 2
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "reLU3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "norm3"
  type: "LRN"
  bottom: "conv3"
  top: "norm3"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "norm3"
  top: "conv4"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 256
    kernel_size: 2
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv4"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 3
  }
}
layer {
  name: "norm4"
  type: "LRN"
  bottom: "pool1"
  top: "norm4"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "dropout2"
  type: "Dropout"
  bottom: "norm4"
  top: "norm4"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "norm4"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 256
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "reLU4"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "dropout3"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 256
    weight_filler {
      type: "xavier"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "reLU5"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "dropout4"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 121
    weight_filler {
      type: "xavier"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0425 23:46:43.418277  2388 layer_factory.hpp:74] Creating layer ndsb
I0425 23:46:43.419612  2388 net.cpp:84] Creating Layer ndsb
I0425 23:46:43.419630  2388 net.cpp:338] ndsb -> data
I0425 23:46:43.419661  2388 net.cpp:338] ndsb -> label
I0425 23:46:43.419680  2388 net.cpp:113] Setting up ndsb
I0425 23:46:43.948586  2388 db.cpp:34] Opened lmdb /home/nitini/data_files/48_cross_val_files/cv_training_lmdb
I0425 23:46:44.868765  2388 data_layer.cpp:67] output data size: 256,3,70,70
I0425 23:46:44.868791  2388 data_transformer.cpp:22] Loading mean file from: ./train_all_48_mean.binaryproto
I0425 23:46:45.250766  2388 net.cpp:120] Top shape: 256 3 70 70 (3763200)
I0425 23:46:45.250783  2388 net.cpp:120] Top shape: 256 (256)
I0425 23:46:45.250795  2388 layer_factory.hpp:74] Creating layer conv1
I0425 23:46:45.250818  2388 net.cpp:84] Creating Layer conv1
I0425 23:46:45.250829  2388 net.cpp:380] conv1 <- data
I0425 23:46:45.250847  2388 net.cpp:338] conv1 -> conv1
I0425 23:46:45.250864  2388 net.cpp:113] Setting up conv1
F0425 23:46:45.429028  2406 data_transformer.cpp:59] Check failed: datum_height == data_mean_.height() (70 vs. 48) 
*** Check failure stack trace: ***
    @     0x2ab697375e6d  (unknown)
    @     0x2ab697377ced  (unknown)
    @     0x2ab697375a5c  (unknown)
    @     0x2ab69737863e  (unknown)
    @     0x2ab691366ccd  caffe::DataTransformer<>::Transform()
    @     0x2ab691368065  caffe::DataTransformer<>::Transform()
    @     0x2ab6913a3568  caffe::DataLayer<>::InternalThreadEntry()
    @     0x2ab69967524a  (unknown)
    @     0x2ab69c406df5  start_thread
    @     0x2ab69c7111ad  __clone
/var/sge/default/spool/aws-foster-01/job_scripts/61970: line 5:  2388 Aborted                 caffe train --solver=/home/nitini/eas_499_code/network_architectures/seaNet_solver_all.prototxt
