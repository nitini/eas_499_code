I0426 21:43:53.624577  5190 caffe.cpp:113] Use GPU with device ID 0
I0426 21:43:54.112121  5190 caffe.cpp:121] Starting Optimization
I0426 21:43:54.112277  5190 solver.cpp:32] Initializing solver from parameters: 
test_iter: 64
test_interval: 1000
base_lr: 0.001
display: 500
max_iter: 50000
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
stepsize: 100000
solver_mode: GPU
net: "/home/nitini/eas_499_code/network_architectures/11_seaNet_train_test.prototxt"
I0426 21:43:54.112320  5190 solver.cpp:70] Creating training net from net file: /home/nitini/eas_499_code/network_architectures/11_seaNet_train_test.prototxt
I0426 21:43:54.206686  5190 net.cpp:257] The NetState phase (0) differed from the phase (1) specified by a rule in layer ndsb
I0426 21:43:54.206723  5190 net.cpp:257] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0426 21:43:54.206939  5190 net.cpp:42] Initializing net from parameters: 
name: "SeaNet"
state {
  phase: TRAIN
}
layer {
  name: "ndsb"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
    mean_file: "./train_all_48_mean.binaryproto"
  }
  data_param {
    source: "/home/nitini/data_files/cross_val_files/cv_training_lmdb"
    batch_size: 256
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 128
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "reLU1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 128
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "reLU2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "dropout1"
  type: "Dropout"
  bottom: "norm2"
  top: "norm2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 256
    kernel_size: 2
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "reLU3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "norm3"
  type: "LRN"
  bottom: "conv3"
  top: "norm3"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "norm3"
  top: "conv4"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 256
    kernel_size: 2
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv4"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 3
  }
}
layer {
  name: "norm4"
  type: "LRN"
  bottom: "pool1"
  top: "norm4"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "dropout2"
  type: "Dropout"
  bottom: "norm4"
  top: "norm4"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "norm4"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 256
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "reLU4"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "dropout3"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 256
    weight_filler {
      type: "xavier"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "reLU5"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "dropout4"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 121
    weight_filler {
      type: "xavier"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0426 21:43:54.207103  5190 layer_factory.hpp:74] Creating layer ndsb
I0426 21:43:54.207129  5190 net.cpp:84] Creating Layer ndsb
I0426 21:43:54.207141  5190 net.cpp:338] ndsb -> data
I0426 21:43:54.207177  5190 net.cpp:338] ndsb -> label
I0426 21:43:54.207195  5190 net.cpp:113] Setting up ndsb
I0426 21:43:54.649144  5190 db.cpp:34] Opened lmdb /home/nitini/data_files/cross_val_files/cv_training_lmdb
I0426 21:43:54.680430  5190 data_layer.cpp:67] output data size: 256,3,48,48
I0426 21:43:54.680454  5190 data_transformer.cpp:22] Loading mean file from: ./train_all_48_mean.binaryproto
I0426 21:43:54.779690  5190 net.cpp:120] Top shape: 256 3 48 48 (1769472)
I0426 21:43:54.779706  5190 net.cpp:120] Top shape: 256 (256)
I0426 21:43:54.779714  5190 layer_factory.hpp:74] Creating layer conv1
I0426 21:43:54.779734  5190 net.cpp:84] Creating Layer conv1
I0426 21:43:54.779744  5190 net.cpp:380] conv1 <- data
I0426 21:43:54.779762  5190 net.cpp:338] conv1 -> conv1
I0426 21:43:54.779778  5190 net.cpp:113] Setting up conv1
I0426 21:43:56.741762  5190 net.cpp:120] Top shape: 256 128 23 23 (17334272)
I0426 21:43:56.741811  5190 layer_factory.hpp:74] Creating layer reLU1
I0426 21:43:56.741830  5190 net.cpp:84] Creating Layer reLU1
I0426 21:43:56.741837  5190 net.cpp:380] reLU1 <- conv1
I0426 21:43:56.741847  5190 net.cpp:327] reLU1 -> conv1 (in-place)
I0426 21:43:56.741860  5190 net.cpp:113] Setting up reLU1
I0426 21:43:56.742024  5190 net.cpp:120] Top shape: 256 128 23 23 (17334272)
I0426 21:43:56.742036  5190 layer_factory.hpp:74] Creating layer norm1
I0426 21:43:56.742050  5190 net.cpp:84] Creating Layer norm1
I0426 21:43:56.742056  5190 net.cpp:380] norm1 <- conv1
I0426 21:43:56.742065  5190 net.cpp:338] norm1 -> norm1
I0426 21:43:56.742076  5190 net.cpp:113] Setting up norm1
I0426 21:43:56.742089  5190 net.cpp:120] Top shape: 256 128 23 23 (17334272)
I0426 21:43:56.742095  5190 layer_factory.hpp:74] Creating layer conv2
I0426 21:43:56.742110  5190 net.cpp:84] Creating Layer conv2
I0426 21:43:56.742116  5190 net.cpp:380] conv2 <- norm1
I0426 21:43:56.742125  5190 net.cpp:338] conv2 -> conv2
I0426 21:43:56.742136  5190 net.cpp:113] Setting up conv2
I0426 21:43:56.743474  5190 net.cpp:120] Top shape: 256 128 21 21 (14450688)
I0426 21:43:56.743491  5190 layer_factory.hpp:74] Creating layer reLU2
I0426 21:43:56.743501  5190 net.cpp:84] Creating Layer reLU2
I0426 21:43:56.743507  5190 net.cpp:380] reLU2 <- conv2
I0426 21:43:56.743515  5190 net.cpp:327] reLU2 -> conv2 (in-place)
I0426 21:43:56.743523  5190 net.cpp:113] Setting up reLU2
I0426 21:43:56.743587  5190 net.cpp:120] Top shape: 256 128 21 21 (14450688)
I0426 21:43:56.743594  5190 layer_factory.hpp:74] Creating layer norm2
I0426 21:43:56.743603  5190 net.cpp:84] Creating Layer norm2
I0426 21:43:56.743609  5190 net.cpp:380] norm2 <- conv2
I0426 21:43:56.743616  5190 net.cpp:338] norm2 -> norm2
I0426 21:43:56.743625  5190 net.cpp:113] Setting up norm2
I0426 21:43:56.743634  5190 net.cpp:120] Top shape: 256 128 21 21 (14450688)
I0426 21:43:56.743640  5190 layer_factory.hpp:74] Creating layer dropout1
I0426 21:43:56.743656  5190 net.cpp:84] Creating Layer dropout1
I0426 21:43:56.743695  5190 net.cpp:380] dropout1 <- norm2
I0426 21:43:56.743703  5190 net.cpp:327] dropout1 -> norm2 (in-place)
I0426 21:43:56.743715  5190 net.cpp:113] Setting up dropout1
I0426 21:43:56.743727  5190 net.cpp:120] Top shape: 256 128 21 21 (14450688)
I0426 21:43:56.743733  5190 layer_factory.hpp:74] Creating layer conv3
I0426 21:43:56.743744  5190 net.cpp:84] Creating Layer conv3
I0426 21:43:56.743752  5190 net.cpp:380] conv3 <- norm2
I0426 21:43:56.743762  5190 net.cpp:338] conv3 -> conv3
I0426 21:43:56.743770  5190 net.cpp:113] Setting up conv3
I0426 21:43:56.744933  5190 net.cpp:120] Top shape: 256 256 10 10 (6553600)
I0426 21:43:56.744952  5190 layer_factory.hpp:74] Creating layer reLU3
I0426 21:43:56.744962  5190 net.cpp:84] Creating Layer reLU3
I0426 21:43:56.744968  5190 net.cpp:380] reLU3 <- conv3
I0426 21:43:56.744976  5190 net.cpp:327] reLU3 -> conv3 (in-place)
I0426 21:43:56.744983  5190 net.cpp:113] Setting up reLU3
I0426 21:43:56.745030  5190 net.cpp:120] Top shape: 256 256 10 10 (6553600)
I0426 21:43:56.745038  5190 layer_factory.hpp:74] Creating layer norm3
I0426 21:43:56.745046  5190 net.cpp:84] Creating Layer norm3
I0426 21:43:56.745053  5190 net.cpp:380] norm3 <- conv3
I0426 21:43:56.745060  5190 net.cpp:338] norm3 -> norm3
I0426 21:43:56.745069  5190 net.cpp:113] Setting up norm3
I0426 21:43:56.745076  5190 net.cpp:120] Top shape: 256 256 10 10 (6553600)
I0426 21:43:56.745082  5190 layer_factory.hpp:74] Creating layer conv4
I0426 21:43:56.745091  5190 net.cpp:84] Creating Layer conv4
I0426 21:43:56.745097  5190 net.cpp:380] conv4 <- norm3
I0426 21:43:56.745105  5190 net.cpp:338] conv4 -> conv4
I0426 21:43:56.745113  5190 net.cpp:113] Setting up conv4
I0426 21:43:56.747360  5190 net.cpp:120] Top shape: 256 256 9 9 (5308416)
I0426 21:43:56.747377  5190 layer_factory.hpp:74] Creating layer pool1
I0426 21:43:56.747397  5190 net.cpp:84] Creating Layer pool1
I0426 21:43:56.747403  5190 net.cpp:380] pool1 <- conv4
I0426 21:43:56.747412  5190 net.cpp:338] pool1 -> pool1
I0426 21:43:56.747421  5190 net.cpp:113] Setting up pool1
I0426 21:43:56.747586  5190 net.cpp:120] Top shape: 256 256 3 3 (589824)
I0426 21:43:56.747598  5190 layer_factory.hpp:74] Creating layer norm4
I0426 21:43:56.747608  5190 net.cpp:84] Creating Layer norm4
I0426 21:43:56.747614  5190 net.cpp:380] norm4 <- pool1
I0426 21:43:56.747622  5190 net.cpp:338] norm4 -> norm4
I0426 21:43:56.747632  5190 net.cpp:113] Setting up norm4
I0426 21:43:56.747640  5190 net.cpp:120] Top shape: 256 256 3 3 (589824)
I0426 21:43:56.747647  5190 layer_factory.hpp:74] Creating layer dropout2
I0426 21:43:56.747653  5190 net.cpp:84] Creating Layer dropout2
I0426 21:43:56.747659  5190 net.cpp:380] dropout2 <- norm4
I0426 21:43:56.747666  5190 net.cpp:327] dropout2 -> norm4 (in-place)
I0426 21:43:56.747674  5190 net.cpp:113] Setting up dropout2
I0426 21:43:56.747683  5190 net.cpp:120] Top shape: 256 256 3 3 (589824)
I0426 21:43:56.747687  5190 layer_factory.hpp:74] Creating layer ip1
I0426 21:43:56.747699  5190 net.cpp:84] Creating Layer ip1
I0426 21:43:56.747705  5190 net.cpp:380] ip1 <- norm4
I0426 21:43:56.747712  5190 net.cpp:338] ip1 -> ip1
I0426 21:43:56.747725  5190 net.cpp:113] Setting up ip1
I0426 21:43:56.752970  5190 net.cpp:120] Top shape: 256 256 (65536)
I0426 21:43:56.752990  5190 layer_factory.hpp:74] Creating layer reLU4
I0426 21:43:56.753000  5190 net.cpp:84] Creating Layer reLU4
I0426 21:43:56.753006  5190 net.cpp:380] reLU4 <- ip1
I0426 21:43:56.753015  5190 net.cpp:327] reLU4 -> ip1 (in-place)
I0426 21:43:56.753022  5190 net.cpp:113] Setting up reLU4
I0426 21:43:56.753077  5190 net.cpp:120] Top shape: 256 256 (65536)
I0426 21:43:56.753085  5190 layer_factory.hpp:74] Creating layer dropout3
I0426 21:43:56.753093  5190 net.cpp:84] Creating Layer dropout3
I0426 21:43:56.753099  5190 net.cpp:380] dropout3 <- ip1
I0426 21:43:56.753106  5190 net.cpp:327] dropout3 -> ip1 (in-place)
I0426 21:43:56.753114  5190 net.cpp:113] Setting up dropout3
I0426 21:43:56.753126  5190 net.cpp:120] Top shape: 256 256 (65536)
I0426 21:43:56.753146  5190 layer_factory.hpp:74] Creating layer ip2
I0426 21:43:56.753157  5190 net.cpp:84] Creating Layer ip2
I0426 21:43:56.753164  5190 net.cpp:380] ip2 <- ip1
I0426 21:43:56.753172  5190 net.cpp:338] ip2 -> ip2
I0426 21:43:56.753181  5190 net.cpp:113] Setting up ip2
I0426 21:43:56.753661  5190 net.cpp:120] Top shape: 256 256 (65536)
I0426 21:43:56.753675  5190 layer_factory.hpp:74] Creating layer reLU5
I0426 21:43:56.753684  5190 net.cpp:84] Creating Layer reLU5
I0426 21:43:56.753690  5190 net.cpp:380] reLU5 <- ip2
I0426 21:43:56.753696  5190 net.cpp:327] reLU5 -> ip2 (in-place)
I0426 21:43:56.753703  5190 net.cpp:113] Setting up reLU5
I0426 21:43:56.753753  5190 net.cpp:120] Top shape: 256 256 (65536)
I0426 21:43:56.753762  5190 layer_factory.hpp:74] Creating layer dropout4
I0426 21:43:56.753769  5190 net.cpp:84] Creating Layer dropout4
I0426 21:43:56.753775  5190 net.cpp:380] dropout4 <- ip2
I0426 21:43:56.753784  5190 net.cpp:327] dropout4 -> ip2 (in-place)
I0426 21:43:56.753793  5190 net.cpp:113] Setting up dropout4
I0426 21:43:56.753801  5190 net.cpp:120] Top shape: 256 256 (65536)
I0426 21:43:56.753808  5190 layer_factory.hpp:74] Creating layer ip3
I0426 21:43:56.753816  5190 net.cpp:84] Creating Layer ip3
I0426 21:43:56.753821  5190 net.cpp:380] ip3 <- ip2
I0426 21:43:56.753829  5190 net.cpp:338] ip3 -> ip3
I0426 21:43:56.753837  5190 net.cpp:113] Setting up ip3
I0426 21:43:56.754118  5190 net.cpp:120] Top shape: 256 121 (30976)
I0426 21:43:56.754130  5190 layer_factory.hpp:74] Creating layer loss
I0426 21:43:56.754142  5190 net.cpp:84] Creating Layer loss
I0426 21:43:56.754148  5190 net.cpp:380] loss <- ip3
I0426 21:43:56.754155  5190 net.cpp:380] loss <- label
I0426 21:43:56.754164  5190 net.cpp:338] loss -> loss
I0426 21:43:56.754176  5190 net.cpp:113] Setting up loss
I0426 21:43:56.754187  5190 layer_factory.hpp:74] Creating layer loss
I0426 21:43:56.754328  5190 net.cpp:120] Top shape: (1)
I0426 21:43:56.754338  5190 net.cpp:122]     with loss weight 1
I0426 21:43:56.754369  5190 net.cpp:167] loss needs backward computation.
I0426 21:43:56.754376  5190 net.cpp:167] ip3 needs backward computation.
I0426 21:43:56.754381  5190 net.cpp:167] dropout4 needs backward computation.
I0426 21:43:56.754386  5190 net.cpp:167] reLU5 needs backward computation.
I0426 21:43:56.754390  5190 net.cpp:167] ip2 needs backward computation.
I0426 21:43:56.754395  5190 net.cpp:167] dropout3 needs backward computation.
I0426 21:43:56.754400  5190 net.cpp:167] reLU4 needs backward computation.
I0426 21:43:56.754405  5190 net.cpp:167] ip1 needs backward computation.
I0426 21:43:56.754410  5190 net.cpp:167] dropout2 needs backward computation.
I0426 21:43:56.754415  5190 net.cpp:167] norm4 needs backward computation.
I0426 21:43:56.754420  5190 net.cpp:167] pool1 needs backward computation.
I0426 21:43:56.754425  5190 net.cpp:167] conv4 needs backward computation.
I0426 21:43:56.754429  5190 net.cpp:167] norm3 needs backward computation.
I0426 21:43:56.754434  5190 net.cpp:167] reLU3 needs backward computation.
I0426 21:43:56.754439  5190 net.cpp:167] conv3 needs backward computation.
I0426 21:43:56.754444  5190 net.cpp:167] dropout1 needs backward computation.
I0426 21:43:56.754449  5190 net.cpp:167] norm2 needs backward computation.
I0426 21:43:56.754454  5190 net.cpp:167] reLU2 needs backward computation.
I0426 21:43:56.754459  5190 net.cpp:167] conv2 needs backward computation.
I0426 21:43:56.754464  5190 net.cpp:167] norm1 needs backward computation.
I0426 21:43:56.754469  5190 net.cpp:167] reLU1 needs backward computation.
I0426 21:43:56.754473  5190 net.cpp:167] conv1 needs backward computation.
I0426 21:43:56.754479  5190 net.cpp:169] ndsb does not need backward computation.
I0426 21:43:56.754483  5190 net.cpp:205] This network produces output loss
I0426 21:43:56.754498  5190 net.cpp:447] Collecting Learning Rate and Weight Decay.
I0426 21:43:56.754508  5190 net.cpp:217] Network initialization done.
I0426 21:43:56.754513  5190 net.cpp:218] Memory required for data: 554952708
I0426 21:43:56.817293  5190 solver.cpp:154] Creating test net (#0) specified by net file: /home/nitini/eas_499_code/network_architectures/11_seaNet_train_test.prototxt
I0426 21:43:56.817359  5190 net.cpp:257] The NetState phase (1) differed from the phase (0) specified by a rule in layer ndsb
I0426 21:43:56.817600  5190 net.cpp:42] Initializing net from parameters: 
name: "SeaNet"
state {
  phase: TEST
}
layer {
  name: "ndsb"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
    mean_file: "./train_all_48_mean.binaryproto"
  }
  data_param {
    source: "/home/nitini/data_files/cross_val_files/cv_holdout_lmdb"
    batch_size: 256
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 128
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "reLU1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 128
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "reLU2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "dropout1"
  type: "Dropout"
  bottom: "norm2"
  top: "norm2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 256
    kernel_size: 2
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "reLU3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "norm3"
  type: "LRN"
  bottom: "conv3"
  top: "norm3"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "norm3"
  top: "conv4"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 256
    kernel_size: 2
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv4"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 3
  }
}
layer {
  name: "norm4"
  type: "LRN"
  bottom: "pool1"
  top: "norm4"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "dropout2"
  type: "Dropout"
  bottom: "norm4"
  top: "norm4"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "norm4"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 256
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "reLU4"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "dropout3"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 256
    weight_filler {
      type: "xavier"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "reLU5"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "dropout4"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 121
    weight_filler {
      type: "xavier"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0426 21:43:56.817730  5190 layer_factory.hpp:74] Creating layer ndsb
I0426 21:43:56.817744  5190 net.cpp:84] Creating Layer ndsb
I0426 21:43:56.817751  5190 net.cpp:338] ndsb -> data
I0426 21:43:56.817762  5190 net.cpp:338] ndsb -> label
I0426 21:43:56.817771  5190 net.cpp:113] Setting up ndsb
I0426 21:43:57.134706  5190 db.cpp:34] Opened lmdb /home/nitini/data_files/cross_val_files/cv_holdout_lmdb
I0426 21:43:57.165374  5190 data_layer.cpp:67] output data size: 256,3,48,48
I0426 21:43:57.165390  5190 data_transformer.cpp:22] Loading mean file from: ./train_all_48_mean.binaryproto
I0426 21:43:57.231384  5190 net.cpp:120] Top shape: 256 3 48 48 (1769472)
I0426 21:43:57.231397  5190 net.cpp:120] Top shape: 256 (256)
I0426 21:43:57.231410  5190 layer_factory.hpp:74] Creating layer label_ndsb_1_split
I0426 21:43:57.231425  5190 net.cpp:84] Creating Layer label_ndsb_1_split
I0426 21:43:57.231431  5190 net.cpp:380] label_ndsb_1_split <- label
I0426 21:43:57.231441  5190 net.cpp:338] label_ndsb_1_split -> label_ndsb_1_split_0
I0426 21:43:57.231452  5190 net.cpp:338] label_ndsb_1_split -> label_ndsb_1_split_1
I0426 21:43:57.231462  5190 net.cpp:113] Setting up label_ndsb_1_split
I0426 21:43:57.231472  5190 net.cpp:120] Top shape: 256 (256)
I0426 21:43:57.231477  5190 net.cpp:120] Top shape: 256 (256)
I0426 21:43:57.231482  5190 layer_factory.hpp:74] Creating layer conv1
I0426 21:43:57.231494  5190 net.cpp:84] Creating Layer conv1
I0426 21:43:57.231500  5190 net.cpp:380] conv1 <- data
I0426 21:43:57.231509  5190 net.cpp:338] conv1 -> conv1
I0426 21:43:57.231518  5190 net.cpp:113] Setting up conv1
I0426 21:43:57.231856  5190 net.cpp:120] Top shape: 256 128 23 23 (17334272)
I0426 21:43:57.231875  5190 layer_factory.hpp:74] Creating layer reLU1
I0426 21:43:57.231885  5190 net.cpp:84] Creating Layer reLU1
I0426 21:43:57.231892  5190 net.cpp:380] reLU1 <- conv1
I0426 21:43:57.231899  5190 net.cpp:327] reLU1 -> conv1 (in-place)
I0426 21:43:57.231907  5190 net.cpp:113] Setting up reLU1
I0426 21:43:57.232043  5190 net.cpp:120] Top shape: 256 128 23 23 (17334272)
I0426 21:43:57.232056  5190 layer_factory.hpp:74] Creating layer norm1
I0426 21:43:57.232069  5190 net.cpp:84] Creating Layer norm1
I0426 21:43:57.232074  5190 net.cpp:380] norm1 <- conv1
I0426 21:43:57.232081  5190 net.cpp:338] norm1 -> norm1
I0426 21:43:57.232090  5190 net.cpp:113] Setting up norm1
I0426 21:43:57.232100  5190 net.cpp:120] Top shape: 256 128 23 23 (17334272)
I0426 21:43:57.232105  5190 layer_factory.hpp:74] Creating layer conv2
I0426 21:43:57.232113  5190 net.cpp:84] Creating Layer conv2
I0426 21:43:57.232120  5190 net.cpp:380] conv2 <- norm1
I0426 21:43:57.232126  5190 net.cpp:338] conv2 -> conv2
I0426 21:43:57.232136  5190 net.cpp:113] Setting up conv2
I0426 21:43:57.233641  5190 net.cpp:120] Top shape: 256 128 21 21 (14450688)
I0426 21:43:57.233660  5190 layer_factory.hpp:74] Creating layer reLU2
I0426 21:43:57.233670  5190 net.cpp:84] Creating Layer reLU2
I0426 21:43:57.233676  5190 net.cpp:380] reLU2 <- conv2
I0426 21:43:57.233686  5190 net.cpp:327] reLU2 -> conv2 (in-place)
I0426 21:43:57.233695  5190 net.cpp:113] Setting up reLU2
I0426 21:43:57.233758  5190 net.cpp:120] Top shape: 256 128 21 21 (14450688)
I0426 21:43:57.233767  5190 layer_factory.hpp:74] Creating layer norm2
I0426 21:43:57.233777  5190 net.cpp:84] Creating Layer norm2
I0426 21:43:57.233783  5190 net.cpp:380] norm2 <- conv2
I0426 21:43:57.233790  5190 net.cpp:338] norm2 -> norm2
I0426 21:43:57.233798  5190 net.cpp:113] Setting up norm2
I0426 21:43:57.233813  5190 net.cpp:120] Top shape: 256 128 21 21 (14450688)
I0426 21:43:57.233844  5190 layer_factory.hpp:74] Creating layer dropout1
I0426 21:43:57.233855  5190 net.cpp:84] Creating Layer dropout1
I0426 21:43:57.233861  5190 net.cpp:380] dropout1 <- norm2
I0426 21:43:57.233871  5190 net.cpp:327] dropout1 -> norm2 (in-place)
I0426 21:43:57.233880  5190 net.cpp:113] Setting up dropout1
I0426 21:43:57.233888  5190 net.cpp:120] Top shape: 256 128 21 21 (14450688)
I0426 21:43:57.233894  5190 layer_factory.hpp:74] Creating layer conv3
I0426 21:43:57.233902  5190 net.cpp:84] Creating Layer conv3
I0426 21:43:57.233911  5190 net.cpp:380] conv3 <- norm2
I0426 21:43:57.233917  5190 net.cpp:338] conv3 -> conv3
I0426 21:43:57.233928  5190 net.cpp:113] Setting up conv3
I0426 21:43:57.235296  5190 net.cpp:120] Top shape: 256 256 10 10 (6553600)
I0426 21:43:57.235322  5190 layer_factory.hpp:74] Creating layer reLU3
I0426 21:43:57.235333  5190 net.cpp:84] Creating Layer reLU3
I0426 21:43:57.235339  5190 net.cpp:380] reLU3 <- conv3
I0426 21:43:57.235349  5190 net.cpp:327] reLU3 -> conv3 (in-place)
I0426 21:43:57.235357  5190 net.cpp:113] Setting up reLU3
I0426 21:43:57.235414  5190 net.cpp:120] Top shape: 256 256 10 10 (6553600)
I0426 21:43:57.235422  5190 layer_factory.hpp:74] Creating layer norm3
I0426 21:43:57.235430  5190 net.cpp:84] Creating Layer norm3
I0426 21:43:57.235436  5190 net.cpp:380] norm3 <- conv3
I0426 21:43:57.235443  5190 net.cpp:338] norm3 -> norm3
I0426 21:43:57.235451  5190 net.cpp:113] Setting up norm3
I0426 21:43:57.235460  5190 net.cpp:120] Top shape: 256 256 10 10 (6553600)
I0426 21:43:57.235466  5190 layer_factory.hpp:74] Creating layer conv4
I0426 21:43:57.235476  5190 net.cpp:84] Creating Layer conv4
I0426 21:43:57.235483  5190 net.cpp:380] conv4 <- norm3
I0426 21:43:57.235493  5190 net.cpp:338] conv4 -> conv4
I0426 21:43:57.235502  5190 net.cpp:113] Setting up conv4
I0426 21:43:57.237987  5190 net.cpp:120] Top shape: 256 256 9 9 (5308416)
I0426 21:43:57.238005  5190 layer_factory.hpp:74] Creating layer pool1
I0426 21:43:57.238016  5190 net.cpp:84] Creating Layer pool1
I0426 21:43:57.238023  5190 net.cpp:380] pool1 <- conv4
I0426 21:43:57.238030  5190 net.cpp:338] pool1 -> pool1
I0426 21:43:57.238041  5190 net.cpp:113] Setting up pool1
I0426 21:43:57.238108  5190 net.cpp:120] Top shape: 256 256 3 3 (589824)
I0426 21:43:57.238119  5190 layer_factory.hpp:74] Creating layer norm4
I0426 21:43:57.238129  5190 net.cpp:84] Creating Layer norm4
I0426 21:43:57.238136  5190 net.cpp:380] norm4 <- pool1
I0426 21:43:57.238143  5190 net.cpp:338] norm4 -> norm4
I0426 21:43:57.238152  5190 net.cpp:113] Setting up norm4
I0426 21:43:57.238160  5190 net.cpp:120] Top shape: 256 256 3 3 (589824)
I0426 21:43:57.238165  5190 layer_factory.hpp:74] Creating layer dropout2
I0426 21:43:57.238173  5190 net.cpp:84] Creating Layer dropout2
I0426 21:43:57.238178  5190 net.cpp:380] dropout2 <- norm4
I0426 21:43:57.238188  5190 net.cpp:327] dropout2 -> norm4 (in-place)
I0426 21:43:57.238195  5190 net.cpp:113] Setting up dropout2
I0426 21:43:57.238204  5190 net.cpp:120] Top shape: 256 256 3 3 (589824)
I0426 21:43:57.238209  5190 layer_factory.hpp:74] Creating layer ip1
I0426 21:43:57.238226  5190 net.cpp:84] Creating Layer ip1
I0426 21:43:57.238231  5190 net.cpp:380] ip1 <- norm4
I0426 21:43:57.238242  5190 net.cpp:338] ip1 -> ip1
I0426 21:43:57.238251  5190 net.cpp:113] Setting up ip1
I0426 21:43:57.242885  5190 net.cpp:120] Top shape: 256 256 (65536)
I0426 21:43:57.242907  5190 layer_factory.hpp:74] Creating layer reLU4
I0426 21:43:57.242915  5190 net.cpp:84] Creating Layer reLU4
I0426 21:43:57.242921  5190 net.cpp:380] reLU4 <- ip1
I0426 21:43:57.242929  5190 net.cpp:327] reLU4 -> ip1 (in-place)
I0426 21:43:57.242936  5190 net.cpp:113] Setting up reLU4
I0426 21:43:57.243087  5190 net.cpp:120] Top shape: 256 256 (65536)
I0426 21:43:57.243098  5190 layer_factory.hpp:74] Creating layer dropout3
I0426 21:43:57.243106  5190 net.cpp:84] Creating Layer dropout3
I0426 21:43:57.243113  5190 net.cpp:380] dropout3 <- ip1
I0426 21:43:57.243124  5190 net.cpp:327] dropout3 -> ip1 (in-place)
I0426 21:43:57.243146  5190 net.cpp:113] Setting up dropout3
I0426 21:43:57.243156  5190 net.cpp:120] Top shape: 256 256 (65536)
I0426 21:43:57.243161  5190 layer_factory.hpp:74] Creating layer ip2
I0426 21:43:57.243170  5190 net.cpp:84] Creating Layer ip2
I0426 21:43:57.243175  5190 net.cpp:380] ip2 <- ip1
I0426 21:43:57.243186  5190 net.cpp:338] ip2 -> ip2
I0426 21:43:57.243196  5190 net.cpp:113] Setting up ip2
I0426 21:43:57.243783  5190 net.cpp:120] Top shape: 256 256 (65536)
I0426 21:43:57.243798  5190 layer_factory.hpp:74] Creating layer reLU5
I0426 21:43:57.243808  5190 net.cpp:84] Creating Layer reLU5
I0426 21:43:57.243814  5190 net.cpp:380] reLU5 <- ip2
I0426 21:43:57.243821  5190 net.cpp:327] reLU5 -> ip2 (in-place)
I0426 21:43:57.243829  5190 net.cpp:113] Setting up reLU5
I0426 21:43:57.243890  5190 net.cpp:120] Top shape: 256 256 (65536)
I0426 21:43:57.243898  5190 layer_factory.hpp:74] Creating layer dropout4
I0426 21:43:57.243907  5190 net.cpp:84] Creating Layer dropout4
I0426 21:43:57.243913  5190 net.cpp:380] dropout4 <- ip2
I0426 21:43:57.243921  5190 net.cpp:327] dropout4 -> ip2 (in-place)
I0426 21:43:57.243928  5190 net.cpp:113] Setting up dropout4
I0426 21:43:57.243937  5190 net.cpp:120] Top shape: 256 256 (65536)
I0426 21:43:57.243942  5190 layer_factory.hpp:74] Creating layer ip3
I0426 21:43:57.243950  5190 net.cpp:84] Creating Layer ip3
I0426 21:43:57.243955  5190 net.cpp:380] ip3 <- ip2
I0426 21:43:57.243964  5190 net.cpp:338] ip3 -> ip3
I0426 21:43:57.243973  5190 net.cpp:113] Setting up ip3
I0426 21:43:57.244246  5190 net.cpp:120] Top shape: 256 121 (30976)
I0426 21:43:57.244257  5190 layer_factory.hpp:74] Creating layer ip3_ip3_0_split
I0426 21:43:57.244271  5190 net.cpp:84] Creating Layer ip3_ip3_0_split
I0426 21:43:57.244278  5190 net.cpp:380] ip3_ip3_0_split <- ip3
I0426 21:43:57.244285  5190 net.cpp:338] ip3_ip3_0_split -> ip3_ip3_0_split_0
I0426 21:43:57.244295  5190 net.cpp:338] ip3_ip3_0_split -> ip3_ip3_0_split_1
I0426 21:43:57.244307  5190 net.cpp:113] Setting up ip3_ip3_0_split
I0426 21:43:57.244315  5190 net.cpp:120] Top shape: 256 121 (30976)
I0426 21:43:57.244321  5190 net.cpp:120] Top shape: 256 121 (30976)
I0426 21:43:57.244326  5190 layer_factory.hpp:74] Creating layer accuracy
I0426 21:43:57.244339  5190 net.cpp:84] Creating Layer accuracy
I0426 21:43:57.244348  5190 net.cpp:380] accuracy <- ip3_ip3_0_split_0
I0426 21:43:57.244354  5190 net.cpp:380] accuracy <- label_ndsb_1_split_0
I0426 21:43:57.244361  5190 net.cpp:338] accuracy -> accuracy
I0426 21:43:57.244369  5190 net.cpp:113] Setting up accuracy
I0426 21:43:57.244380  5190 net.cpp:120] Top shape: (1)
I0426 21:43:57.244385  5190 layer_factory.hpp:74] Creating layer loss
I0426 21:43:57.244393  5190 net.cpp:84] Creating Layer loss
I0426 21:43:57.244398  5190 net.cpp:380] loss <- ip3_ip3_0_split_1
I0426 21:43:57.244407  5190 net.cpp:380] loss <- label_ndsb_1_split_1
I0426 21:43:57.244415  5190 net.cpp:338] loss -> loss
I0426 21:43:57.244421  5190 net.cpp:113] Setting up loss
I0426 21:43:57.244429  5190 layer_factory.hpp:74] Creating layer loss
I0426 21:43:57.244576  5190 net.cpp:120] Top shape: (1)
I0426 21:43:57.244586  5190 net.cpp:122]     with loss weight 1
I0426 21:43:57.244602  5190 net.cpp:167] loss needs backward computation.
I0426 21:43:57.244608  5190 net.cpp:169] accuracy does not need backward computation.
I0426 21:43:57.244613  5190 net.cpp:167] ip3_ip3_0_split needs backward computation.
I0426 21:43:57.244618  5190 net.cpp:167] ip3 needs backward computation.
I0426 21:43:57.244623  5190 net.cpp:167] dropout4 needs backward computation.
I0426 21:43:57.244628  5190 net.cpp:167] reLU5 needs backward computation.
I0426 21:43:57.244632  5190 net.cpp:167] ip2 needs backward computation.
I0426 21:43:57.244637  5190 net.cpp:167] dropout3 needs backward computation.
I0426 21:43:57.244642  5190 net.cpp:167] reLU4 needs backward computation.
I0426 21:43:57.244647  5190 net.cpp:167] ip1 needs backward computation.
I0426 21:43:57.244652  5190 net.cpp:167] dropout2 needs backward computation.
I0426 21:43:57.244673  5190 net.cpp:167] norm4 needs backward computation.
I0426 21:43:57.244678  5190 net.cpp:167] pool1 needs backward computation.
I0426 21:43:57.244683  5190 net.cpp:167] conv4 needs backward computation.
I0426 21:43:57.244688  5190 net.cpp:167] norm3 needs backward computation.
I0426 21:43:57.244696  5190 net.cpp:167] reLU3 needs backward computation.
I0426 21:43:57.244701  5190 net.cpp:167] conv3 needs backward computation.
I0426 21:43:57.244706  5190 net.cpp:167] dropout1 needs backward computation.
I0426 21:43:57.244711  5190 net.cpp:167] norm2 needs backward computation.
I0426 21:43:57.244716  5190 net.cpp:167] reLU2 needs backward computation.
I0426 21:43:57.244720  5190 net.cpp:167] conv2 needs backward computation.
I0426 21:43:57.244725  5190 net.cpp:167] norm1 needs backward computation.
I0426 21:43:57.244730  5190 net.cpp:167] reLU1 needs backward computation.
I0426 21:43:57.244735  5190 net.cpp:167] conv1 needs backward computation.
I0426 21:43:57.244740  5190 net.cpp:169] label_ndsb_1_split does not need backward computation.
I0426 21:43:57.244745  5190 net.cpp:169] ndsb does not need backward computation.
I0426 21:43:57.244750  5190 net.cpp:205] This network produces output accuracy
I0426 21:43:57.244755  5190 net.cpp:205] This network produces output loss
I0426 21:43:57.244774  5190 net.cpp:447] Collecting Learning Rate and Weight Decay.
I0426 21:43:57.244786  5190 net.cpp:217] Network initialization done.
I0426 21:43:57.244791  5190 net.cpp:218] Memory required for data: 555202568
I0426 21:43:57.244907  5190 solver.cpp:42] Solver scaffolding done.
I0426 21:43:57.244947  5190 solver.cpp:222] Solving SeaNet
I0426 21:43:57.244953  5190 solver.cpp:223] Learning Rate Policy: step
I0426 21:43:57.244963  5190 solver.cpp:266] Iteration 0, Testing net (#0)
I0426 21:44:03.414319  5190 solver.cpp:315]     Test net output #0: accuracy = 0.00402832
I0426 21:44:03.445783  5190 solver.cpp:315]     Test net output #1: loss = 4.79772 (* 1 = 4.79772 loss)
I0426 21:44:03.565450  5190 solver.cpp:189] Iteration 0, loss = 4.78913
I0426 21:44:03.565498  5190 solver.cpp:204]     Train net output #0: loss = 4.78913 (* 1 = 4.78913 loss)
I0426 21:44:03.565521  5190 solver.cpp:464] Iteration 0, lr = 0.001
