I0426 20:02:46.280926  2416 caffe.cpp:113] Use GPU with device ID 0
I0426 20:02:53.464480  2416 caffe.cpp:121] Starting Optimization
I0426 20:02:53.495841  2416 solver.cpp:32] Initializing solver from parameters: 
test_iter: 64
test_interval: 1000
base_lr: 0.001
display: 500
max_iter: 50000
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
stepsize: 100000
solver_mode: GPU
net: "/home/nitini/eas_499_code/network_architectures/11_seaNet_train_test.prototxt"
I0426 20:02:53.495882  2416 solver.cpp:70] Creating training net from net file: /home/nitini/eas_499_code/network_architectures/11_seaNet_train_test.prototxt
I0426 20:02:53.651588  2416 net.cpp:257] The NetState phase (0) differed from the phase (1) specified by a rule in layer ndsb
I0426 20:02:53.651628  2416 net.cpp:257] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0426 20:02:53.651829  2416 net.cpp:42] Initializing net from parameters: 
name: "SeaNet"
state {
  phase: TRAIN
}
layer {
  name: "ndsb"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
    mean_file: "./train_all_48_mean.binaryproto"
  }
  data_param {
    source: "/home/nitini/data_files/48_cross_val_files/cv_training_lmdb"
    batch_size: 256
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 128
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "reLU1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 128
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "reLU2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "dropout1"
  type: "Dropout"
  bottom: "norm2"
  top: "norm2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 256
    kernel_size: 2
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "reLU3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "norm3"
  type: "LRN"
  bottom: "conv3"
  top: "norm3"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "norm3"
  top: "conv4"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 256
    kernel_size: 2
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv4"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 3
  }
}
layer {
  name: "norm4"
  type: "LRN"
  bottom: "pool1"
  top: "norm4"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "dropout2"
  type: "Dropout"
  bottom: "norm4"
  top: "norm4"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "norm4"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 256
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "reLU4"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "dropout3"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 256
    weight_filler {
      type: "xavier"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "reLU5"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "dropout4"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 121
    weight_filler {
      type: "xavier"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0426 20:02:53.651974  2416 layer_factory.hpp:74] Creating layer ndsb
I0426 20:02:53.653023  2416 net.cpp:84] Creating Layer ndsb
I0426 20:02:53.653039  2416 net.cpp:338] ndsb -> data
I0426 20:02:53.653070  2416 net.cpp:338] ndsb -> label
I0426 20:02:53.653087  2416 net.cpp:113] Setting up ndsb
I0426 20:02:54.184092  2416 db.cpp:34] Opened lmdb /home/nitini/data_files/48_cross_val_files/cv_training_lmdb
I0426 20:02:55.267040  2416 data_layer.cpp:67] output data size: 256,3,70,70
I0426 20:02:55.267065  2416 data_transformer.cpp:22] Loading mean file from: ./train_all_48_mean.binaryproto
I0426 20:02:55.589675  2416 net.cpp:120] Top shape: 256 3 70 70 (3763200)
I0426 20:02:55.589691  2416 net.cpp:120] Top shape: 256 (256)
I0426 20:02:55.589701  2416 layer_factory.hpp:74] Creating layer conv1
I0426 20:02:55.589723  2416 net.cpp:84] Creating Layer conv1
I0426 20:02:55.589735  2416 net.cpp:380] conv1 <- data
I0426 20:02:55.589753  2416 net.cpp:338] conv1 -> conv1
I0426 20:02:55.589769  2416 net.cpp:113] Setting up conv1
F0426 20:02:55.969043  2432 data_transformer.cpp:59] Check failed: datum_height == data_mean_.height() (70 vs. 48) 
*** Check failure stack trace: ***
    @     0x2b4b69a40e6d  (unknown)
    @     0x2b4b69a42ced  (unknown)
    @     0x2b4b69a40a5c  (unknown)
    @     0x2b4b69a4363e  (unknown)
    @     0x2b4b63a31ccd  caffe::DataTransformer<>::Transform()
    @     0x2b4b63a33065  caffe::DataTransformer<>::Transform()
    @     0x2b4b63a6e568  caffe::DataLayer<>::InternalThreadEntry()
    @     0x2b4b6bd4024a  (unknown)
    @     0x2b4b6ead1df5  start_thread
    @     0x2b4b6eddc1ad  __clone
/var/sge/default/spool/aws-foster-02/job_scripts/62028: line 5:  2416 Aborted                 caffe train --solver=/home/nitini/eas_499_code/network_architectures/seaNet_solver_all.prototxt
