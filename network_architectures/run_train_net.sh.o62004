I0426 14:58:59.844156  2398 caffe.cpp:113] Use GPU with device ID 0
I0426 14:59:18.187641  2398 caffe.cpp:121] Starting Optimization
I0426 14:59:18.219519  2398 solver.cpp:32] Initializing solver from parameters: 
test_iter: 64
test_interval: 1000
base_lr: 0.01
display: 500
max_iter: 50000
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
stepsize: 50000
solver_mode: GPU
net: "/home/nitini/eas_499_code/network_architectures/13_seaNet_train_test.prototxt"
I0426 14:59:18.219568  2398 solver.cpp:70] Creating training net from net file: /home/nitini/eas_499_code/network_architectures/13_seaNet_train_test.prototxt
I0426 14:59:18.376461  2398 net.cpp:257] The NetState phase (0) differed from the phase (1) specified by a rule in layer ndsb
I0426 14:59:18.376499  2398 net.cpp:257] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0426 14:59:18.376725  2398 net.cpp:42] Initializing net from parameters: 
name: "SeaNet"
state {
  phase: TRAIN
}
layer {
  name: "ndsb"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
    mean_file: "./train_all_48_mean.binaryproto"
  }
  data_param {
    source: "/home/nitini/data_files/cross_val_files/cv_training_lmdb"
    batch_size: 256
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 128
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "reLU1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 128
    kernel_size: 4
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "reLU2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "dropout1"
  type: "Dropout"
  bottom: "norm2"
  top: "norm2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 128
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "reLU3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "norm3"
  type: "LRN"
  bottom: "conv3"
  top: "norm3"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "norm3"
  top: "conv4"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 128
    kernel_size: 2
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv4"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 3
  }
}
layer {
  name: "norm4"
  type: "LRN"
  bottom: "pool1"
  top: "norm4"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "dropout2"
  type: "Dropout"
  bottom: "norm4"
  top: "norm4"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "norm4"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "reLU4"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "dropout3"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 256
    weight_filler {
      type: "xavier"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "reLU5"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "dropout4"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 121
    weight_filler {
      type: "xavier"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0426 14:59:18.376891  2398 layer_factory.hpp:74] Creating layer ndsb
I0426 14:59:18.378029  2398 net.cpp:84] Creating Layer ndsb
I0426 14:59:18.378046  2398 net.cpp:338] ndsb -> data
I0426 14:59:18.378082  2398 net.cpp:338] ndsb -> label
I0426 14:59:18.378100  2398 net.cpp:113] Setting up ndsb
I0426 14:59:18.909390  2398 db.cpp:34] Opened lmdb /home/nitini/data_files/cross_val_files/cv_training_lmdb
I0426 14:59:19.674653  2398 data_layer.cpp:67] output data size: 256,3,48,48
I0426 14:59:19.674685  2398 data_transformer.cpp:22] Loading mean file from: ./train_all_48_mean.binaryproto
I0426 14:59:20.520257  2398 net.cpp:120] Top shape: 256 3 48 48 (1769472)
I0426 14:59:20.520289  2398 net.cpp:120] Top shape: 256 (256)
I0426 14:59:20.520301  2398 layer_factory.hpp:74] Creating layer conv1
I0426 14:59:20.520330  2398 net.cpp:84] Creating Layer conv1
I0426 14:59:20.520341  2398 net.cpp:380] conv1 <- data
I0426 14:59:20.520364  2398 net.cpp:338] conv1 -> conv1
I0426 14:59:20.520385  2398 net.cpp:113] Setting up conv1
I0426 14:59:26.845340  2398 net.cpp:120] Top shape: 256 128 44 44 (63438848)
I0426 14:59:26.876341  2398 layer_factory.hpp:74] Creating layer reLU1
I0426 14:59:26.876365  2398 net.cpp:84] Creating Layer reLU1
I0426 14:59:26.876374  2398 net.cpp:380] reLU1 <- conv1
I0426 14:59:26.876384  2398 net.cpp:327] reLU1 -> conv1 (in-place)
I0426 14:59:26.876397  2398 net.cpp:113] Setting up reLU1
I0426 14:59:26.877135  2398 net.cpp:120] Top shape: 256 128 44 44 (63438848)
I0426 14:59:26.877147  2398 layer_factory.hpp:74] Creating layer norm1
I0426 14:59:26.877161  2398 net.cpp:84] Creating Layer norm1
I0426 14:59:26.877167  2398 net.cpp:380] norm1 <- conv1
I0426 14:59:26.877177  2398 net.cpp:338] norm1 -> norm1
I0426 14:59:26.877188  2398 net.cpp:113] Setting up norm1
I0426 14:59:26.877202  2398 net.cpp:120] Top shape: 256 128 44 44 (63438848)
I0426 14:59:26.877208  2398 layer_factory.hpp:74] Creating layer conv2
I0426 14:59:26.877223  2398 net.cpp:84] Creating Layer conv2
I0426 14:59:26.877228  2398 net.cpp:380] conv2 <- norm1
I0426 14:59:26.877236  2398 net.cpp:338] conv2 -> conv2
I0426 14:59:26.877248  2398 net.cpp:113] Setting up conv2
I0426 14:59:26.879891  2398 net.cpp:120] Top shape: 256 128 41 41 (55083008)
I0426 14:59:26.879909  2398 layer_factory.hpp:74] Creating layer reLU2
I0426 14:59:26.879919  2398 net.cpp:84] Creating Layer reLU2
I0426 14:59:26.879925  2398 net.cpp:380] reLU2 <- conv2
I0426 14:59:26.879931  2398 net.cpp:327] reLU2 -> conv2 (in-place)
I0426 14:59:26.879940  2398 net.cpp:113] Setting up reLU2
I0426 14:59:26.879987  2398 net.cpp:120] Top shape: 256 128 41 41 (55083008)
I0426 14:59:26.879995  2398 layer_factory.hpp:74] Creating layer norm2
I0426 14:59:26.880004  2398 net.cpp:84] Creating Layer norm2
I0426 14:59:26.880010  2398 net.cpp:380] norm2 <- conv2
I0426 14:59:26.880017  2398 net.cpp:338] norm2 -> norm2
I0426 14:59:26.880026  2398 net.cpp:113] Setting up norm2
I0426 14:59:26.880035  2398 net.cpp:120] Top shape: 256 128 41 41 (55083008)
I0426 14:59:26.880040  2398 layer_factory.hpp:74] Creating layer dropout1
I0426 14:59:26.880058  2398 net.cpp:84] Creating Layer dropout1
I0426 14:59:26.880091  2398 net.cpp:380] dropout1 <- norm2
I0426 14:59:26.880100  2398 net.cpp:327] dropout1 -> norm2 (in-place)
I0426 14:59:26.880110  2398 net.cpp:113] Setting up dropout1
I0426 14:59:26.880123  2398 net.cpp:120] Top shape: 256 128 41 41 (55083008)
I0426 14:59:26.880130  2398 layer_factory.hpp:74] Creating layer conv3
I0426 14:59:26.880141  2398 net.cpp:84] Creating Layer conv3
I0426 14:59:26.880146  2398 net.cpp:380] conv3 <- norm2
I0426 14:59:26.880154  2398 net.cpp:338] conv3 -> conv3
I0426 14:59:26.880162  2398 net.cpp:113] Setting up conv3
I0426 14:59:26.881669  2398 net.cpp:120] Top shape: 256 128 39 39 (49840128)
I0426 14:59:26.881686  2398 layer_factory.hpp:74] Creating layer reLU3
I0426 14:59:26.881695  2398 net.cpp:84] Creating Layer reLU3
I0426 14:59:26.881701  2398 net.cpp:380] reLU3 <- conv3
I0426 14:59:26.881708  2398 net.cpp:327] reLU3 -> conv3 (in-place)
I0426 14:59:26.881716  2398 net.cpp:113] Setting up reLU3
I0426 14:59:26.881763  2398 net.cpp:120] Top shape: 256 128 39 39 (49840128)
I0426 14:59:26.881772  2398 layer_factory.hpp:74] Creating layer norm3
I0426 14:59:26.881780  2398 net.cpp:84] Creating Layer norm3
I0426 14:59:26.881785  2398 net.cpp:380] norm3 <- conv3
I0426 14:59:26.881793  2398 net.cpp:338] norm3 -> norm3
I0426 14:59:26.881801  2398 net.cpp:113] Setting up norm3
I0426 14:59:26.881809  2398 net.cpp:120] Top shape: 256 128 39 39 (49840128)
I0426 14:59:26.881815  2398 layer_factory.hpp:74] Creating layer conv4
I0426 14:59:26.881824  2398 net.cpp:84] Creating Layer conv4
I0426 14:59:26.881830  2398 net.cpp:380] conv4 <- norm3
I0426 14:59:26.881839  2398 net.cpp:338] conv4 -> conv4
I0426 14:59:26.881849  2398 net.cpp:113] Setting up conv4
I0426 14:59:26.882721  2398 net.cpp:120] Top shape: 256 128 38 38 (47316992)
I0426 14:59:26.882738  2398 layer_factory.hpp:74] Creating layer pool1
I0426 14:59:26.882755  2398 net.cpp:84] Creating Layer pool1
I0426 14:59:26.882762  2398 net.cpp:380] pool1 <- conv4
I0426 14:59:26.882773  2398 net.cpp:338] pool1 -> pool1
I0426 14:59:26.882782  2398 net.cpp:113] Setting up pool1
I0426 14:59:26.882943  2398 net.cpp:120] Top shape: 256 128 13 13 (5537792)
I0426 14:59:26.882956  2398 layer_factory.hpp:74] Creating layer norm4
I0426 14:59:26.882967  2398 net.cpp:84] Creating Layer norm4
I0426 14:59:26.882973  2398 net.cpp:380] norm4 <- pool1
I0426 14:59:26.882982  2398 net.cpp:338] norm4 -> norm4
I0426 14:59:26.882990  2398 net.cpp:113] Setting up norm4
I0426 14:59:26.882998  2398 net.cpp:120] Top shape: 256 128 13 13 (5537792)
I0426 14:59:26.883004  2398 layer_factory.hpp:74] Creating layer dropout2
I0426 14:59:26.883013  2398 net.cpp:84] Creating Layer dropout2
I0426 14:59:26.883018  2398 net.cpp:380] dropout2 <- norm4
I0426 14:59:26.883029  2398 net.cpp:327] dropout2 -> norm4 (in-place)
I0426 14:59:26.883038  2398 net.cpp:113] Setting up dropout2
I0426 14:59:26.883046  2398 net.cpp:120] Top shape: 256 128 13 13 (5537792)
I0426 14:59:26.883051  2398 layer_factory.hpp:74] Creating layer ip1
I0426 14:59:26.883064  2398 net.cpp:84] Creating Layer ip1
I0426 14:59:26.883069  2398 net.cpp:380] ip1 <- norm4
I0426 14:59:26.883079  2398 net.cpp:338] ip1 -> ip1
I0426 14:59:26.883090  2398 net.cpp:113] Setting up ip1
I0426 14:59:26.980602  2398 net.cpp:120] Top shape: 256 512 (131072)
I0426 14:59:26.980659  2398 layer_factory.hpp:74] Creating layer reLU4
I0426 14:59:26.980680  2398 net.cpp:84] Creating Layer reLU4
I0426 14:59:26.980689  2398 net.cpp:380] reLU4 <- ip1
I0426 14:59:26.980700  2398 net.cpp:327] reLU4 -> ip1 (in-place)
I0426 14:59:26.980715  2398 net.cpp:113] Setting up reLU4
I0426 14:59:26.980820  2398 net.cpp:120] Top shape: 256 512 (131072)
I0426 14:59:26.980829  2398 layer_factory.hpp:74] Creating layer dropout3
I0426 14:59:26.980839  2398 net.cpp:84] Creating Layer dropout3
I0426 14:59:26.980845  2398 net.cpp:380] dropout3 <- ip1
I0426 14:59:26.980854  2398 net.cpp:327] dropout3 -> ip1 (in-place)
I0426 14:59:26.980864  2398 net.cpp:113] Setting up dropout3
I0426 14:59:26.980878  2398 net.cpp:120] Top shape: 256 512 (131072)
I0426 14:59:26.980916  2398 layer_factory.hpp:74] Creating layer ip2
I0426 14:59:26.980928  2398 net.cpp:84] Creating Layer ip2
I0426 14:59:26.980933  2398 net.cpp:380] ip2 <- ip1
I0426 14:59:26.980944  2398 net.cpp:338] ip2 -> ip2
I0426 14:59:26.980957  2398 net.cpp:113] Setting up ip2
I0426 14:59:26.982098  2398 net.cpp:120] Top shape: 256 256 (65536)
I0426 14:59:26.982113  2398 layer_factory.hpp:74] Creating layer reLU5
I0426 14:59:26.982121  2398 net.cpp:84] Creating Layer reLU5
I0426 14:59:26.982126  2398 net.cpp:380] reLU5 <- ip2
I0426 14:59:26.982136  2398 net.cpp:327] reLU5 -> ip2 (in-place)
I0426 14:59:26.982143  2398 net.cpp:113] Setting up reLU5
I0426 14:59:26.982208  2398 net.cpp:120] Top shape: 256 256 (65536)
I0426 14:59:26.982218  2398 layer_factory.hpp:74] Creating layer dropout4
I0426 14:59:26.982226  2398 net.cpp:84] Creating Layer dropout4
I0426 14:59:26.982231  2398 net.cpp:380] dropout4 <- ip2
I0426 14:59:26.982240  2398 net.cpp:327] dropout4 -> ip2 (in-place)
I0426 14:59:26.982247  2398 net.cpp:113] Setting up dropout4
I0426 14:59:26.982256  2398 net.cpp:120] Top shape: 256 256 (65536)
I0426 14:59:26.982261  2398 layer_factory.hpp:74] Creating layer ip3
I0426 14:59:26.982270  2398 net.cpp:84] Creating Layer ip3
I0426 14:59:26.982276  2398 net.cpp:380] ip3 <- ip2
I0426 14:59:26.982283  2398 net.cpp:338] ip3 -> ip3
I0426 14:59:26.982291  2398 net.cpp:113] Setting up ip3
I0426 14:59:26.982573  2398 net.cpp:120] Top shape: 256 121 (30976)
I0426 14:59:26.982586  2398 layer_factory.hpp:74] Creating layer loss
I0426 14:59:26.982600  2398 net.cpp:84] Creating Layer loss
I0426 14:59:26.982606  2398 net.cpp:380] loss <- ip3
I0426 14:59:26.982612  2398 net.cpp:380] loss <- label
I0426 14:59:26.982626  2398 net.cpp:338] loss -> loss
I0426 14:59:26.983099  2398 net.cpp:113] Setting up loss
I0426 14:59:26.983119  2398 layer_factory.hpp:74] Creating layer loss
I0426 14:59:26.983263  2398 net.cpp:120] Top shape: (1)
I0426 14:59:26.983273  2398 net.cpp:122]     with loss weight 1
I0426 14:59:26.983315  2398 net.cpp:167] loss needs backward computation.
I0426 14:59:26.983322  2398 net.cpp:167] ip3 needs backward computation.
I0426 14:59:26.983327  2398 net.cpp:167] dropout4 needs backward computation.
I0426 14:59:26.983332  2398 net.cpp:167] reLU5 needs backward computation.
I0426 14:59:26.983336  2398 net.cpp:167] ip2 needs backward computation.
I0426 14:59:26.983341  2398 net.cpp:167] dropout3 needs backward computation.
I0426 14:59:26.983346  2398 net.cpp:167] reLU4 needs backward computation.
I0426 14:59:26.983350  2398 net.cpp:167] ip1 needs backward computation.
I0426 14:59:26.983355  2398 net.cpp:167] dropout2 needs backward computation.
I0426 14:59:26.983361  2398 net.cpp:167] norm4 needs backward computation.
I0426 14:59:26.983366  2398 net.cpp:167] pool1 needs backward computation.
I0426 14:59:26.983371  2398 net.cpp:167] conv4 needs backward computation.
I0426 14:59:26.983376  2398 net.cpp:167] norm3 needs backward computation.
I0426 14:59:26.983381  2398 net.cpp:167] reLU3 needs backward computation.
I0426 14:59:26.983386  2398 net.cpp:167] conv3 needs backward computation.
I0426 14:59:26.983392  2398 net.cpp:167] dropout1 needs backward computation.
I0426 14:59:26.983397  2398 net.cpp:167] norm2 needs backward computation.
I0426 14:59:26.983402  2398 net.cpp:167] reLU2 needs backward computation.
I0426 14:59:26.983407  2398 net.cpp:167] conv2 needs backward computation.
I0426 14:59:26.983412  2398 net.cpp:167] norm1 needs backward computation.
I0426 14:59:26.983417  2398 net.cpp:167] reLU1 needs backward computation.
I0426 14:59:26.983422  2398 net.cpp:167] conv1 needs backward computation.
I0426 14:59:26.983428  2398 net.cpp:169] ndsb does not need backward computation.
I0426 14:59:26.983433  2398 net.cpp:205] This network produces output loss
I0426 14:59:26.983451  2398 net.cpp:447] Collecting Learning Rate and Weight Decay.
I0426 14:59:26.983479  2398 net.cpp:217] Network initialization done.
I0426 14:59:26.983485  2398 net.cpp:218] Memory required for data: 2505959428
I0426 14:59:27.076702  2398 solver.cpp:154] Creating test net (#0) specified by net file: /home/nitini/eas_499_code/network_architectures/13_seaNet_train_test.prototxt
I0426 14:59:27.076769  2398 net.cpp:257] The NetState phase (1) differed from the phase (0) specified by a rule in layer ndsb
I0426 14:59:27.077006  2398 net.cpp:42] Initializing net from parameters: 
name: "SeaNet"
state {
  phase: TEST
}
layer {
  name: "ndsb"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
    mean_file: "./train_all_48_mean.binaryproto"
  }
  data_param {
    source: "/home/nitini/data_files/cross_val_files/cv_holdout_lmdb"
    batch_size: 256
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 128
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "reLU1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 128
    kernel_size: 4
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "reLU2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "dropout1"
  type: "Dropout"
  bottom: "norm2"
  top: "norm2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 128
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "reLU3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "norm3"
  type: "LRN"
  bottom: "conv3"
  top: "norm3"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "norm3"
  top: "conv4"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 128
    kernel_size: 2
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv4"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 3
  }
}
layer {
  name: "norm4"
  type: "LRN"
  bottom: "pool1"
  top: "norm4"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "dropout2"
  type: "Dropout"
  bottom: "norm4"
  top: "norm4"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "norm4"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "reLU4"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "dropout3"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 256
    weight_filler {
      type: "xavier"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "reLU5"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "dropout4"
  type: "Dropout"
  bottom: "ip2"
  top: "ip2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip3"
  type: "InnerProduct"
  bottom: "ip2"
  top: "ip3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 121
    weight_filler {
      type: "xavier"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip3"
  bottom: "label"
  top: "loss"
}
I0426 14:59:27.077152  2398 layer_factory.hpp:74] Creating layer ndsb
I0426 14:59:27.077167  2398 net.cpp:84] Creating Layer ndsb
I0426 14:59:27.077174  2398 net.cpp:338] ndsb -> data
I0426 14:59:27.077185  2398 net.cpp:338] ndsb -> label
I0426 14:59:27.077195  2398 net.cpp:113] Setting up ndsb
I0426 14:59:27.394243  2398 db.cpp:34] Opened lmdb /home/nitini/data_files/cross_val_files/cv_holdout_lmdb
I0426 14:59:27.718757  2398 data_layer.cpp:67] output data size: 256,3,48,48
I0426 14:59:27.718775  2398 data_transformer.cpp:22] Loading mean file from: ./train_all_48_mean.binaryproto
I0426 14:59:28.488880  2398 net.cpp:120] Top shape: 256 3 48 48 (1769472)
I0426 14:59:28.488908  2398 net.cpp:120] Top shape: 256 (256)
I0426 14:59:28.488916  2398 layer_factory.hpp:74] Creating layer label_ndsb_1_split
I0426 14:59:28.488937  2398 net.cpp:84] Creating Layer label_ndsb_1_split
I0426 14:59:28.488945  2398 net.cpp:380] label_ndsb_1_split <- label
I0426 14:59:28.488956  2398 net.cpp:338] label_ndsb_1_split -> label_ndsb_1_split_0
I0426 14:59:28.488972  2398 net.cpp:338] label_ndsb_1_split -> label_ndsb_1_split_1
I0426 14:59:28.488984  2398 net.cpp:113] Setting up label_ndsb_1_split
I0426 14:59:28.488996  2398 net.cpp:120] Top shape: 256 (256)
I0426 14:59:28.489001  2398 net.cpp:120] Top shape: 256 (256)
I0426 14:59:28.489007  2398 layer_factory.hpp:74] Creating layer conv1
I0426 14:59:28.489020  2398 net.cpp:84] Creating Layer conv1
I0426 14:59:28.489027  2398 net.cpp:380] conv1 <- data
I0426 14:59:28.489034  2398 net.cpp:338] conv1 -> conv1
I0426 14:59:28.489045  2398 net.cpp:113] Setting up conv1
I0426 14:59:28.489542  2398 net.cpp:120] Top shape: 256 128 44 44 (63438848)
I0426 14:59:28.489562  2398 layer_factory.hpp:74] Creating layer reLU1
I0426 14:59:28.489574  2398 net.cpp:84] Creating Layer reLU1
I0426 14:59:28.489580  2398 net.cpp:380] reLU1 <- conv1
I0426 14:59:28.489588  2398 net.cpp:327] reLU1 -> conv1 (in-place)
I0426 14:59:28.489596  2398 net.cpp:113] Setting up reLU1
I0426 14:59:28.489742  2398 net.cpp:120] Top shape: 256 128 44 44 (63438848)
I0426 14:59:28.489754  2398 layer_factory.hpp:74] Creating layer norm1
I0426 14:59:28.489769  2398 net.cpp:84] Creating Layer norm1
I0426 14:59:28.489778  2398 net.cpp:380] norm1 <- conv1
I0426 14:59:28.489785  2398 net.cpp:338] norm1 -> norm1
I0426 14:59:28.489794  2398 net.cpp:113] Setting up norm1
I0426 14:59:28.489804  2398 net.cpp:120] Top shape: 256 128 44 44 (63438848)
I0426 14:59:28.489809  2398 layer_factory.hpp:74] Creating layer conv2
I0426 14:59:28.489821  2398 net.cpp:84] Creating Layer conv2
I0426 14:59:28.489826  2398 net.cpp:380] conv2 <- norm1
I0426 14:59:28.489835  2398 net.cpp:338] conv2 -> conv2
I0426 14:59:28.489843  2398 net.cpp:113] Setting up conv2
I0426 14:59:28.492331  2398 net.cpp:120] Top shape: 256 128 41 41 (55083008)
I0426 14:59:28.492348  2398 layer_factory.hpp:74] Creating layer reLU2
I0426 14:59:28.492357  2398 net.cpp:84] Creating Layer reLU2
I0426 14:59:28.492363  2398 net.cpp:380] reLU2 <- conv2
I0426 14:59:28.492372  2398 net.cpp:327] reLU2 -> conv2 (in-place)
I0426 14:59:28.492379  2398 net.cpp:113] Setting up reLU2
I0426 14:59:28.492444  2398 net.cpp:120] Top shape: 256 128 41 41 (55083008)
I0426 14:59:28.492465  2398 layer_factory.hpp:74] Creating layer norm2
I0426 14:59:28.492478  2398 net.cpp:84] Creating Layer norm2
I0426 14:59:28.492485  2398 net.cpp:380] norm2 <- conv2
I0426 14:59:28.492492  2398 net.cpp:338] norm2 -> norm2
I0426 14:59:28.492511  2398 net.cpp:113] Setting up norm2
I0426 14:59:28.492521  2398 net.cpp:120] Top shape: 256 128 41 41 (55083008)
I0426 14:59:28.492552  2398 layer_factory.hpp:74] Creating layer dropout1
I0426 14:59:28.492563  2398 net.cpp:84] Creating Layer dropout1
I0426 14:59:28.492569  2398 net.cpp:380] dropout1 <- norm2
I0426 14:59:28.492576  2398 net.cpp:327] dropout1 -> norm2 (in-place)
I0426 14:59:28.492584  2398 net.cpp:113] Setting up dropout1
I0426 14:59:28.492594  2398 net.cpp:120] Top shape: 256 128 41 41 (55083008)
I0426 14:59:28.492599  2398 layer_factory.hpp:74] Creating layer conv3
I0426 14:59:28.492614  2398 net.cpp:84] Creating Layer conv3
I0426 14:59:28.492619  2398 net.cpp:380] conv3 <- norm2
I0426 14:59:28.492629  2398 net.cpp:338] conv3 -> conv3
I0426 14:59:28.492638  2398 net.cpp:113] Setting up conv3
I0426 14:59:28.494155  2398 net.cpp:120] Top shape: 256 128 39 39 (49840128)
I0426 14:59:28.494174  2398 layer_factory.hpp:74] Creating layer reLU3
I0426 14:59:28.494184  2398 net.cpp:84] Creating Layer reLU3
I0426 14:59:28.494189  2398 net.cpp:380] reLU3 <- conv3
I0426 14:59:28.494199  2398 net.cpp:327] reLU3 -> conv3 (in-place)
I0426 14:59:28.494209  2398 net.cpp:113] Setting up reLU3
I0426 14:59:28.494267  2398 net.cpp:120] Top shape: 256 128 39 39 (49840128)
I0426 14:59:28.494277  2398 layer_factory.hpp:74] Creating layer norm3
I0426 14:59:28.494284  2398 net.cpp:84] Creating Layer norm3
I0426 14:59:28.494290  2398 net.cpp:380] norm3 <- conv3
I0426 14:59:28.494297  2398 net.cpp:338] norm3 -> norm3
I0426 14:59:28.494305  2398 net.cpp:113] Setting up norm3
I0426 14:59:28.494314  2398 net.cpp:120] Top shape: 256 128 39 39 (49840128)
I0426 14:59:28.494319  2398 layer_factory.hpp:74] Creating layer conv4
I0426 14:59:28.494331  2398 net.cpp:84] Creating Layer conv4
I0426 14:59:28.494338  2398 net.cpp:380] conv4 <- norm3
I0426 14:59:28.494346  2398 net.cpp:338] conv4 -> conv4
I0426 14:59:28.494355  2398 net.cpp:113] Setting up conv4
I0426 14:59:28.495182  2398 net.cpp:120] Top shape: 256 128 38 38 (47316992)
I0426 14:59:28.495198  2398 layer_factory.hpp:74] Creating layer pool1
I0426 14:59:28.495213  2398 net.cpp:84] Creating Layer pool1
I0426 14:59:28.495218  2398 net.cpp:380] pool1 <- conv4
I0426 14:59:28.495226  2398 net.cpp:338] pool1 -> pool1
I0426 14:59:28.495234  2398 net.cpp:113] Setting up pool1
I0426 14:59:28.495302  2398 net.cpp:120] Top shape: 256 128 13 13 (5537792)
I0426 14:59:28.495311  2398 layer_factory.hpp:74] Creating layer norm4
I0426 14:59:28.495321  2398 net.cpp:84] Creating Layer norm4
I0426 14:59:28.495327  2398 net.cpp:380] norm4 <- pool1
I0426 14:59:28.495334  2398 net.cpp:338] norm4 -> norm4
I0426 14:59:28.495342  2398 net.cpp:113] Setting up norm4
I0426 14:59:28.495352  2398 net.cpp:120] Top shape: 256 128 13 13 (5537792)
I0426 14:59:28.495357  2398 layer_factory.hpp:74] Creating layer dropout2
I0426 14:59:28.495368  2398 net.cpp:84] Creating Layer dropout2
I0426 14:59:28.495373  2398 net.cpp:380] dropout2 <- norm4
I0426 14:59:28.495379  2398 net.cpp:327] dropout2 -> norm4 (in-place)
I0426 14:59:28.495386  2398 net.cpp:113] Setting up dropout2
I0426 14:59:28.495395  2398 net.cpp:120] Top shape: 256 128 13 13 (5537792)
I0426 14:59:28.495400  2398 layer_factory.hpp:74] Creating layer ip1
I0426 14:59:28.495414  2398 net.cpp:84] Creating Layer ip1
I0426 14:59:28.495419  2398 net.cpp:380] ip1 <- norm4
I0426 14:59:28.495427  2398 net.cpp:338] ip1 -> ip1
I0426 14:59:28.495453  2398 net.cpp:113] Setting up ip1
I0426 14:59:28.593106  2398 net.cpp:120] Top shape: 256 512 (131072)
I0426 14:59:28.593159  2398 layer_factory.hpp:74] Creating layer reLU4
I0426 14:59:28.593178  2398 net.cpp:84] Creating Layer reLU4
I0426 14:59:28.593185  2398 net.cpp:380] reLU4 <- ip1
I0426 14:59:28.593199  2398 net.cpp:327] reLU4 -> ip1 (in-place)
I0426 14:59:28.593212  2398 net.cpp:113] Setting up reLU4
I0426 14:59:28.593523  2398 net.cpp:120] Top shape: 256 512 (131072)
I0426 14:59:28.593535  2398 layer_factory.hpp:74] Creating layer dropout3
I0426 14:59:28.593549  2398 net.cpp:84] Creating Layer dropout3
I0426 14:59:28.593561  2398 net.cpp:380] dropout3 <- ip1
I0426 14:59:28.593601  2398 net.cpp:327] dropout3 -> ip1 (in-place)
I0426 14:59:28.593611  2398 net.cpp:113] Setting up dropout3
I0426 14:59:28.593621  2398 net.cpp:120] Top shape: 256 512 (131072)
I0426 14:59:28.593626  2398 layer_factory.hpp:74] Creating layer ip2
I0426 14:59:28.593641  2398 net.cpp:84] Creating Layer ip2
I0426 14:59:28.593647  2398 net.cpp:380] ip2 <- ip1
I0426 14:59:28.593653  2398 net.cpp:338] ip2 -> ip2
I0426 14:59:28.593665  2398 net.cpp:113] Setting up ip2
I0426 14:59:28.594897  2398 net.cpp:120] Top shape: 256 256 (65536)
I0426 14:59:28.594910  2398 layer_factory.hpp:74] Creating layer reLU5
I0426 14:59:28.594919  2398 net.cpp:84] Creating Layer reLU5
I0426 14:59:28.594925  2398 net.cpp:380] reLU5 <- ip2
I0426 14:59:28.594931  2398 net.cpp:327] reLU5 -> ip2 (in-place)
I0426 14:59:28.594938  2398 net.cpp:113] Setting up reLU5
I0426 14:59:28.595002  2398 net.cpp:120] Top shape: 256 256 (65536)
I0426 14:59:28.595010  2398 layer_factory.hpp:74] Creating layer dropout4
I0426 14:59:28.595018  2398 net.cpp:84] Creating Layer dropout4
I0426 14:59:28.595024  2398 net.cpp:380] dropout4 <- ip2
I0426 14:59:28.595032  2398 net.cpp:327] dropout4 -> ip2 (in-place)
I0426 14:59:28.595041  2398 net.cpp:113] Setting up dropout4
I0426 14:59:28.595048  2398 net.cpp:120] Top shape: 256 256 (65536)
I0426 14:59:28.595054  2398 layer_factory.hpp:74] Creating layer ip3
I0426 14:59:28.595063  2398 net.cpp:84] Creating Layer ip3
I0426 14:59:28.595068  2398 net.cpp:380] ip3 <- ip2
I0426 14:59:28.595077  2398 net.cpp:338] ip3 -> ip3
I0426 14:59:28.595087  2398 net.cpp:113] Setting up ip3
I0426 14:59:28.595360  2398 net.cpp:120] Top shape: 256 121 (30976)
I0426 14:59:28.595373  2398 layer_factory.hpp:74] Creating layer ip3_ip3_0_split
I0426 14:59:28.595381  2398 net.cpp:84] Creating Layer ip3_ip3_0_split
I0426 14:59:28.595386  2398 net.cpp:380] ip3_ip3_0_split <- ip3
I0426 14:59:28.595393  2398 net.cpp:338] ip3_ip3_0_split -> ip3_ip3_0_split_0
I0426 14:59:28.595404  2398 net.cpp:338] ip3_ip3_0_split -> ip3_ip3_0_split_1
I0426 14:59:28.595412  2398 net.cpp:113] Setting up ip3_ip3_0_split
I0426 14:59:28.595422  2398 net.cpp:120] Top shape: 256 121 (30976)
I0426 14:59:28.595443  2398 net.cpp:120] Top shape: 256 121 (30976)
I0426 14:59:28.595448  2398 layer_factory.hpp:74] Creating layer accuracy
I0426 14:59:28.595466  2398 net.cpp:84] Creating Layer accuracy
I0426 14:59:28.595473  2398 net.cpp:380] accuracy <- ip3_ip3_0_split_0
I0426 14:59:28.595479  2398 net.cpp:380] accuracy <- label_ndsb_1_split_0
I0426 14:59:28.595486  2398 net.cpp:338] accuracy -> accuracy
I0426 14:59:28.595495  2398 net.cpp:113] Setting up accuracy
I0426 14:59:28.595507  2398 net.cpp:120] Top shape: (1)
I0426 14:59:28.595512  2398 layer_factory.hpp:74] Creating layer loss
I0426 14:59:28.595522  2398 net.cpp:84] Creating Layer loss
I0426 14:59:28.595528  2398 net.cpp:380] loss <- ip3_ip3_0_split_1
I0426 14:59:28.595533  2398 net.cpp:380] loss <- label_ndsb_1_split_1
I0426 14:59:28.595541  2398 net.cpp:338] loss -> loss
I0426 14:59:28.595548  2398 net.cpp:113] Setting up loss
I0426 14:59:28.595557  2398 layer_factory.hpp:74] Creating layer loss
I0426 14:59:28.595695  2398 net.cpp:120] Top shape: (1)
I0426 14:59:28.595705  2398 net.cpp:122]     with loss weight 1
I0426 14:59:28.595724  2398 net.cpp:167] loss needs backward computation.
I0426 14:59:28.595731  2398 net.cpp:169] accuracy does not need backward computation.
I0426 14:59:28.595736  2398 net.cpp:167] ip3_ip3_0_split needs backward computation.
I0426 14:59:28.595741  2398 net.cpp:167] ip3 needs backward computation.
I0426 14:59:28.595746  2398 net.cpp:167] dropout4 needs backward computation.
I0426 14:59:28.595752  2398 net.cpp:167] reLU5 needs backward computation.
I0426 14:59:28.595756  2398 net.cpp:167] ip2 needs backward computation.
I0426 14:59:28.595760  2398 net.cpp:167] dropout3 needs backward computation.
I0426 14:59:28.595765  2398 net.cpp:167] reLU4 needs backward computation.
I0426 14:59:28.595769  2398 net.cpp:167] ip1 needs backward computation.
I0426 14:59:28.595778  2398 net.cpp:167] dropout2 needs backward computation.
I0426 14:59:28.595803  2398 net.cpp:167] norm4 needs backward computation.
I0426 14:59:28.595810  2398 net.cpp:167] pool1 needs backward computation.
I0426 14:59:28.595819  2398 net.cpp:167] conv4 needs backward computation.
I0426 14:59:28.595825  2398 net.cpp:167] norm3 needs backward computation.
I0426 14:59:28.595830  2398 net.cpp:167] reLU3 needs backward computation.
I0426 14:59:28.595835  2398 net.cpp:167] conv3 needs backward computation.
I0426 14:59:28.595841  2398 net.cpp:167] dropout1 needs backward computation.
I0426 14:59:28.595846  2398 net.cpp:167] norm2 needs backward computation.
I0426 14:59:28.595851  2398 net.cpp:167] reLU2 needs backward computation.
I0426 14:59:28.595856  2398 net.cpp:167] conv2 needs backward computation.
I0426 14:59:28.595861  2398 net.cpp:167] norm1 needs backward computation.
I0426 14:59:28.595866  2398 net.cpp:167] reLU1 needs backward computation.
I0426 14:59:28.595871  2398 net.cpp:167] conv1 needs backward computation.
I0426 14:59:28.595877  2398 net.cpp:169] label_ndsb_1_split does not need backward computation.
I0426 14:59:28.595883  2398 net.cpp:169] ndsb does not need backward computation.
I0426 14:59:28.595888  2398 net.cpp:205] This network produces output accuracy
I0426 14:59:28.595893  2398 net.cpp:205] This network produces output loss
I0426 14:59:28.595917  2398 net.cpp:447] Collecting Learning Rate and Weight Decay.
I0426 14:59:28.595927  2398 net.cpp:217] Network initialization done.
I0426 14:59:28.595932  2398 net.cpp:218] Memory required for data: 2506209288
I0426 14:59:28.596072  2398 solver.cpp:42] Solver scaffolding done.
I0426 14:59:28.596117  2398 solver.cpp:222] Solving SeaNet
I0426 14:59:28.596123  2398 solver.cpp:223] Learning Rate Policy: step
I0426 14:59:28.596132  2398 solver.cpp:266] Iteration 0, Testing net (#0)
I0426 15:00:08.507005  2398 solver.cpp:315]     Test net output #0: accuracy = 0.0114746
I0426 15:00:08.538154  2398 solver.cpp:315]     Test net output #1: loss = 4.7927 (* 1 = 4.7927 loss)
F0426 15:00:08.937947  2398 syncedmem.cpp:51] Check failed: error == cudaSuccess (2 vs. 0)  out of memory
*** Check failure stack trace: ***
    @     0x2b45cdfffe6d  (unknown)
    @     0x2b45ce001ced  (unknown)
    @     0x2b45cdfffa5c  (unknown)
    @     0x2b45ce00263e  (unknown)
    @     0x2b45c80b87bb  caffe::SyncedMemory::mutable_gpu_data()
    @     0x2b45c7fde312  caffe::Blob<>::mutable_gpu_data()
    @     0x2b45c80e9840  caffe::LRNLayer<>::CrossChannelForward_gpu()
    @     0x2b45c80e9609  caffe::LRNLayer<>::Forward_gpu()
    @     0x2b45c809c1bf  caffe::Net<>::ForwardFromTo()
    @     0x2b45c809c5e7  caffe::Net<>::ForwardPrefilled()
    @     0x2b45c80b66d5  caffe::Solver<>::Step()
    @     0x2b45c80b6fbf  caffe::Solver<>::Solve()
    @           0x4073b6  train()
    @           0x4058a1  main
    @     0x2b45d32c6af5  __libc_start_main
    @           0x405e4d  (unknown)
/var/sge/default/spool/aws-foster-02/job_scripts/62004: line 5:  2398 Aborted                 caffe train --solver=/home/nitini/eas_499_code/network_architectures/seaNet_solver_all.prototxt
